nerual nets from the group up - Joe Albahari

input 1 ... n -> input weights  + bias -> total input -> activation function -> output

activation function, non-linear => comparator, is the total input greater than zero?

input 1 * weight 1 + ... + input n * weight n + bias => activation function -> output

ReLU - rectified linear unit
Leaky ReLU - rectified linear unit

Loss (or "cost") is the variable we're trying to minimize
Usually it's the error squared

Leave 20% of data for testing

linqpad
Download / import more samples... YOW2018 down the bottom
